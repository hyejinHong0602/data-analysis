{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11c05a044f27e0cfd9d38927523d86f9fcd1ef02"
   },
   "source": [
    "In this kernel, I will try building a CNN from scratch for multi-class classification for the fruits dataset. In this dataset, we have a total of 55244 images which are divided into two folders - training set of 41322 images and testing set of 13877 images. The size of the given images is 100 * 100. We have 81 classes of fruits.\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "8ae6b0f622f3371e629d3bc916d881b11b7a5bf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete!\n",
      "Training set size :  67692\n",
      "Testing set size :  22688\n"
     ]
    }
   ],
   "source": [
    "# First, we are going to load the file names and their respective target labels into numpy array! \n",
    "from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "\n",
    "train_dir = 'C:\\\\Users\\\\hhz20\\\\Desktop\\\\data_analysis\\\\kaggle\\\\fruits-360\\\\Training'\n",
    "test_dir = 'C:\\\\Users\\\\hhz20\\\\Desktop\\\\data_analysis\\\\kaggle\\\\fruits-360\\\\Test'\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np.array(data['target'])\n",
    "    target_labels = np.array(data['target_names'])\n",
    "    return files,targets,target_labels\n",
    "    \n",
    "x_train, y_train,target_labels = load_dataset(train_dir)\n",
    "x_test, y_test,_ = load_dataset(test_dir)\n",
    "print('Loading complete!')\n",
    "\n",
    "print('Training set size : ' , x_train.shape[0])\n",
    "print('Testing set size : ', x_test.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f09063e1d62bcf1fe28eb840ee93fd95fcfdb6ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's confirm the number of classes :p\n",
    "no_of_classes = len(np.unique(y_train))\n",
    "no_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "27cf6d93da0e851e97f341be0dd7060eba55ce0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 27  73  68 114 117  80  72   2  90 130]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:10])\n",
    "# target labels are numbers corresponding to class label. We need to change them to a vector of 81 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "299c14e9e0a8fc0e5fde71bf144a012dee02160b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train,no_of_classes)\n",
    "y_test = np_utils.to_categorical(y_test,no_of_classes)\n",
    "y_train[0] # Note that only one element has value 1(corresponding to its label) and others are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "726a160122475e7d47e0c00bf617b7a5c0838ba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vaildation X :  (7000,)\n",
      "Vaildation y : (7000, 131)\n",
      "Test X :  (15688,)\n",
      "Test y :  (15688, 131)\n"
     ]
    }
   ],
   "source": [
    "# Now, we have to divide the validation set into test and validation set\n",
    "x_test,x_valid = x_test[7000:],x_test[:7000]\n",
    "y_test,y_vaild = y_test[7000:],y_test[:7000]\n",
    "print('Vaildation X : ',x_valid.shape)\n",
    "print('Vaildation y :',y_vaild.shape)\n",
    "print('Test X : ',x_test.shape)\n",
    "print('Test y : ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "aa2cbb8c65386cef369abc49c7a4c1da3957d08e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hhz20\\\\Desktop\\\\data_analysis\\\\kaggle\\\\fruits-360\\\\Training\\\\Cherry 2\\\\r_263_100.jpg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]\n",
    "# training data is just file names of images. We need to convert them into pixel matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b3ab256679300f07d026017477092ebc5c71594",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We just have the file names in the x set. Let's load the images and convert them into array.\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "def convert_image_to_array(files):\n",
    "    images_as_array=[]\n",
    "    for file in files:\n",
    "        # Convert to Numpy Array\n",
    "        images_as_array.append(img_to_array(load_img(file)))\n",
    "    return images_as_array\n",
    "\n",
    "x_train = np.array(convert_image_to_array(x_train))\n",
    "print('Training set shape : ',x_train.shape)\n",
    "\n",
    "x_valid = np.array(convert_image_to_array(x_valid))\n",
    "print('Validation set shape : ',x_valid.shape)\n",
    "\n",
    "x_test = np.array(convert_image_to_array(x_test))\n",
    "print('Test set shape : ',x_test.shape)\n",
    "\n",
    "print('1st training image shape ',x_train[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "fb44a8c41693a8e14332cc734dd2ced9d5582986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st training image as array [[[255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  ...\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]]\n",
      "\n",
      " [[255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  ...\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]]\n",
      "\n",
      " [[255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  ...\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  ...\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]]\n",
      "\n",
      " [[255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  ...\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]]\n",
      "\n",
      " [[255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  ...\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]\n",
      "  [255. 255. 255.]]]\n"
     ]
    }
   ],
   "source": [
    "print('1st training image as array',x_train[0]) # don't worry if you see only 255s..\n",
    "# there are elements will other values too :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "a9bbd173bdc8646ebafeeb31e492c80199683c72"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.57 GiB for an array with shape (67692, 100, 100, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c614863a13e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# time to re-scale so that all the pixel values lie within 0 to 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.57 GiB for an array with shape (67692, 100, 100, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "# time to re-scale so that all the pixel values lie within 0 to 1\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_valid = x_valid.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "4cb2bff712654024215ef79585dba06dd0114396",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x360 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's visualize the first 10 training images!\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize =(30,5))\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(2,5,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(np.squeeze(x_train[i]))\n",
    "# Yummy fruits ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d14849fa59a55c57dfdd9622a4d9bcc90de8971b"
   },
   "outputs": [],
   "source": [
    "#Simple CNN from scratch - we are using 3 Conv layers followed by maxpooling layers.\n",
    "# At the end we add dropout, flatten and some fully connected layers(Dense).\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.layers import Activation, Dense, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 16, kernel_size = 2,input_shape=(100,100,3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters = 32,kernel_size = 2,activation= 'relu',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters = 64,kernel_size = 2,activation= 'relu',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters = 128,kernel_size = 2,activation= 'relu',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(150))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(81,activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dff4d3931ad4ff2748d7681a9767007877694b12"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print('Compiled!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ac38a4408414793435002ff4af1ab25bbf082a8"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'cnn_from_scratch_fruits.hdf5', verbose = 1, save_best_only = True)\n",
    "\n",
    "history = model.fit(x_train,y_train,\n",
    "        batch_size = 32,\n",
    "        epochs=30,\n",
    "        validation_data=(x_valid, y_vaild),\n",
    "        callbacks = [checkpointer],\n",
    "        verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03edb7196563f35ee053d3306f594ece4f50139b"
   },
   "outputs": [],
   "source": [
    "# load the weights that yielded the best validation accuracy\n",
    "model.load_weights('cnn_from_scratch_fruits.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b9df1b9ce5cf750cff33b8ab975acee72c6aa4d"
   },
   "outputs": [],
   "source": [
    "# evaluate and print test accuracy\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('\\n', 'Test accuracy:', score[1])\n",
    "#98% accuracy !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d45b1a4b675f0c1189e8cbdd0876badac6fa1d6e"
   },
   "outputs": [],
   "source": [
    "# Let's visualize test prediction.\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# plot a random sample of test images, their predicted labels, and ground truth\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "for i, idx in enumerate(np.random.choice(x_test.shape[0], size=16, replace=False)):\n",
    "    ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(x_test[idx]))\n",
    "    pred_idx = np.argmax(y_pred[idx])\n",
    "    true_idx = np.argmax(y_test[idx])\n",
    "    ax.set_title(\"{} ({})\".format(target_labels[pred_idx], target_labels[true_idx]),\n",
    "                 color=(\"green\" if pred_idx == true_idx else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4754c40e12c2070c3721e46fba5465df9719151"
   },
   "outputs": [],
   "source": [
    "#Finally lets visualize the loss and accuracy wrt epochs\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(1)  \n",
    "   \n",
    " # summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    " # summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
